{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 39209\n",
      "Number of testing examples = 12630\n",
      "Image data shape = 32\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "%run helpers.py\n",
    "%matplotlib inline\n",
    "\n",
    "training_file = 'data/train.p'\n",
    "testing_file = 'data/test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "signnames = pd.read_csv('signnames.csv')\n",
    "\n",
    "n_train = len(X_train)\n",
    "n_test = len(X_test)\n",
    "image_shape = X_train.shape[1]\n",
    "n_classes = len(set(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape =  (39209, 32, 32, 3)\n",
      "Number of channels in each image =  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data shape = \", X_train.shape)\n",
    "print(\"Number of channels in each image = \", X_train.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "X_train_gray = np.empty( [n_train, image_shape, image_shape], dtype = np.int32)\n",
    "for i, img in enumerate(X_train):\n",
    "    X_train_gray[i,:,:] = cv2.equalizeHist(grayscale(img), (0, 254) )\n",
    "# Test data\n",
    "X_test_gray = np.empty( [n_test, image_shape, image_shape], dtype = np.int32)\n",
    "for i, img in enumerate(X_test):\n",
    "    X_test_gray[i,:,:] = cv2.equalizeHist(grayscale(img), (0, 254) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_gray, X_valid_gray, y_train, y_valid = train_test_split(\n",
    "    X_train_gray,\n",
    "    y_train,\n",
    "    test_size=0.10,\n",
    "    random_state=1973)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(y_train)\n",
    "train_labels = encoder.transform(y_train)\n",
    "valid_labels = encoder.transform(y_valid)\n",
    "test_labels = encoder.transform(y_test)\n",
    "\n",
    "# Change to float32, so it can be multiplied against the features in TensorFlow, which are float32\n",
    "train_labels = train_labels.astype(np.float32)\n",
    "valid_labels = valid_labels.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35288, 32, 32)\n",
      "(3921, 32, 32)\n",
      "(12630, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_gray.shape)\n",
    "print(X_valid_gray.shape)\n",
    "print(X_test_gray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_gray_flat = flatten_all_gray(X_train_gray)\n",
    "X_valid_gray_flat = flatten_all_gray(X_valid_gray)\n",
    "X_test_gray_flat = flatten_all_gray(X_test_gray)\n",
    "\n",
    "X_train_gray_flat = normalize_grayscale(X_train_gray_flat)\n",
    "X_valid_gray_flat = normalize_grayscale(X_valid_gray_flat)\n",
    "X_test_gray_flat = normalize_grayscale(X_test_gray_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.variables import Variable\n",
    "\n",
    "def LeNet(x):\n",
    "    x = tf.reshape(x, (-1, 32, 32, 1))\n",
    "    # Convolution layer 1. The output shape should be 28x28x6.\n",
    "    x=conv_layer(input=x, num_input_channels=1, filter_size=5, num_filters=6, stride=1, padding='VALID')\n",
    "    # Activation 1. Your choice of activation function.\n",
    "    x=tf.nn.relu(x)\n",
    "    # Pooling layer 1. The output shape should be 14x14x6.\n",
    "    x=tf.nn.max_pool(value=x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    # Convolution layer 2. The output shape should be 10x10x16.\n",
    "    x=conv_layer(input=x, num_input_channels=6, filter_size=5, num_filters=16, stride=1, padding='VALID')\n",
    "    # Activation 2. Your choice of activation function.\n",
    "    x=tf.nn.relu(x)\n",
    "    # Pooling layer 2. The output shape should be 5x5x16.\n",
    "    x=tf.nn.max_pool(value=x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    # Flatten layer. Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The easiest way to do is by using tf.contrib.layers.flatten, which is already imported for you.\n",
    "    x=tf.contrib.layers.flatten(x)\n",
    "    # Fully connected layer 1. This should have 120 outputs.\n",
    "    x=fully_connected_layer(x, 400, 120)\n",
    "    # Activation 3. Your choice of activation function.\n",
    "    x=tf.nn.relu(x)\n",
    "    # Fully connected layer 2. This should have 10 outputs.\n",
    "    x=fully_connected_layer(x, 120, 43)\n",
    "    # Return the result of the last fully connected layer.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_count = 32 * 32\n",
    "batch_size=100\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "TRAIN_DIR = 'models/logs/'\n",
    "\n",
    "# Important!!\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, features_count], name='X')\n",
    "y = tf.placeholder(tf.float32, [None, n_classes], name='y') \n",
    "# Need these when restoring model checkpoint:\n",
    "tf.add_to_collection(\"X\",X)\n",
    "tf.add_to_collection(\"y\",y)\n",
    "\n",
    "train_feed_dict = { X: X_train_gray_flat, y: train_labels}\n",
    "valid_feed_dict = { X: X_valid_gray_flat, y: valid_labels}\n",
    "test_feed_dict  = { X: X_test_gray_flat,  y: test_labels}\n",
    "\n",
    "with tf.name_scope('cnn'):\n",
    "    output = LeNet(X)\n",
    "    tf.add_to_collection('output',output) ## Need this when restoring model checkpoint\n",
    "    output_probability = tf.nn.softmax(output)\n",
    "    predicted_class = tf.argmax(output_probability, dimension=1)\n",
    "\n",
    "is_correct_prediction = tf.equal(tf.argmax(output_probability, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n",
    "# For tensorboard\n",
    "tf.scalar_summary(\"accuracy\", accuracy)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y * tf.log(output_probability), reduction_indices=1, name='cross_entropy')\n",
    "loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "# For tensorboard\n",
    "tf.scalar_summary(\"loss\", loss)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "merged = tf.merge_all_summaries()\n",
    "writer = tf.train.SummaryWriter(TRAIN_DIR)\n",
    "saver = tf.train.Saver()\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[n.name for n in tf.get_default_graph().as_graph_def().node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches = int(math.ceil(len(X_train_gray_flat)/batch_size))\n",
    "\n",
    "# Clear out tensorboard logs before training\n",
    "[ os.remove(TRAIN_DIR + '/' + f) for f in os.listdir(TRAIN_DIR) ]\n",
    "\n",
    "checkpoint_file = os.path.join(TRAIN_DIR, 'checkpoint')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    for step in range(epochs):\n",
    "        \n",
    "        for batch_i, batch in enumerate(range(batches)):\n",
    "        \n",
    "            batch_start = batch_i*batch_size\n",
    "            batch_features = X_train_gray_flat[batch_start:batch_start + batch_size]\n",
    "            batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "\n",
    "            _, l = sess.run(\n",
    "                [train_op, loss],\n",
    "                feed_dict={X: batch_features, y: batch_labels})\n",
    "\n",
    "            if not batch_i % 50:\n",
    "                summary, acc = sess.run([merged, accuracy], feed_dict=valid_feed_dict)    \n",
    "                writer.add_summary(summary, step)\n",
    "\n",
    "        # Write a checkpoint\n",
    "        saver.save(sess, checkpoint_file, global_step = step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5*5*48"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
