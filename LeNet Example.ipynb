{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LeNet Architecture\n",
    "\n",
    "HINTS for layers:\n",
    "\n",
    "    Convolutional layers:\n",
    "\n",
    "    tf.nn.conv2d\n",
    "    tf.nn.max_pool\n",
    "\n",
    "    For preparing the convolutional layer output for the\n",
    "    fully connected layers.\n",
    "\n",
    "    tf.contrib.flatten\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "\n",
    "# NOTE: Feel free to change these.\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))\n",
    "\n",
    "def conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   stride,             # stride size\n",
    "                   padding):           # padding 'VALID' or 'SAME'\n",
    "    \n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "    weights = new_weights(shape=shape)\n",
    "    biases = new_biases(length=num_filters)\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, stride, stride, 1],\n",
    "                         padding=padding)\n",
    "    layer += biases\n",
    "    return layer\n",
    "\n",
    "def fully_connected_layer(input,              # The previous layer.\n",
    "                   num_inputs,                # the number of inputs\n",
    "                   num_outputs):              # the number of outputs\n",
    "    \n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution layer 1. The output shape should be 28x28x6.\n",
    "\n",
    "Activation 1. Your choice of activation function.\n",
    "\n",
    "Pooling layer 1. The output shape should be 14x14x6.\n",
    "\n",
    "Convolution layer 2. The output shape should be 10x10x16.\n",
    "\n",
    "Activation 2. Your choice of activation function.\n",
    "\n",
    "Pooling layer 2. The output shape should be 5x5x16.\n",
    "\n",
    "Flatten layer. Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The easiest way to do is by using tf.contrib.layers.flatten, which is already imported for you.\n",
    "\n",
    "Fully connected layer 1. This should have 120 outputs.\n",
    "\n",
    "Activation 3. Your choice of activation function.\n",
    "\n",
    "Fully connected layer 2. This should have 10 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LeNet architecture:\n",
    "# INPUT -> CONV -> ACT -> POOL -> CONV -> ACT -> POOL -> FLATTEN -> FC -> ACT -> FC\n",
    "#\n",
    "# Don't worry about anything else in the file too much, all you have to do is\n",
    "# create the LeNet and return the result of the last fully connected layer.\n",
    "def LeNet(x):\n",
    "    # Reshape from 2D to 4D. This prepares the data for\n",
    "    # convolutional and pooling layers.\n",
    "    x = tf.reshape(x, (-1, 28, 28, 1))\n",
    "    # Squish values from 0-255 to 0-1.\n",
    "    x /= 255.\n",
    "    # Resize to 32x32.\n",
    "    x = tf.image.resize_images(x, (32, 32))\n",
    "    # Convolution layer 1. The output shape should be 28x28x6.\n",
    "    x=conv_layer(input=x, num_input_channels=1, filter_size=5, num_filters=6, stride=1, padding='VALID')\n",
    "    # Activation 1. Your choice of activation function.\n",
    "    x=tf.nn.relu(x)\n",
    "    # Pooling layer 1. The output shape should be 14x14x6.\n",
    "    x=tf.nn.max_pool(value=x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    # Convolution layer 2. The output shape should be 10x10x16.\n",
    "    x=conv_layer(input=x, num_input_channels=6, filter_size=5, num_filters=16, stride=1, padding='VALID')\n",
    "    # Activation 2. Your choice of activation function.\n",
    "    x=tf.nn.relu(x)\n",
    "    # Pooling layer 2. The output shape should be 5x5x16.\n",
    "    x=tf.nn.max_pool(value=x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    # Flatten layer. Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The easiest way to do is by using tf.contrib.layers.flatten, which is already imported for you.\n",
    "    x=tf.contrib.layers.flatten(x)\n",
    "    # Fully connected layer 1. This should have 120 outputs.\n",
    "    x=fully_connected_layer(x, 400, 120)\n",
    "    # Activation 3. Your choice of activation function.\n",
    "    x=tf.nn.relu(x)\n",
    "    # Fully connected layer 2. This should have 10 outputs.\n",
    "    x=fully_connected_layer(x, 120, 10)\n",
    "    # Return the result of the last fully connected layer.\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "EPOCH 1 ...\n",
      "Validation loss = 0.5514993079197712\n",
      "Validation accuracy = 0.8303285256410257\n",
      "EPOCH 2 ...\n",
      "Validation loss = 0.37542294672666454\n",
      "Validation accuracy = 0.8888221153846154\n",
      "EPOCH 3 ...\n",
      "Validation loss = 0.30777175839130694\n",
      "Validation accuracy = 0.9104567307692307\n",
      "EPOCH 4 ...\n",
      "Validation loss = 0.27229593818386394\n",
      "Validation accuracy = 0.9198717948717948\n",
      "EPOCH 5 ...\n",
      "Validation loss = 0.24423356774525765\n",
      "Validation accuracy = 0.9268830128205128\n",
      "EPOCH 6 ...\n",
      "Validation loss = 0.2304763797766123\n",
      "Validation accuracy = 0.9320913461538461\n",
      "EPOCH 7 ...\n",
      "Validation loss = 0.19904011946458083\n",
      "Validation accuracy = 0.9427083333333334\n",
      "EPOCH 8 ...\n",
      "Validation loss = 0.20208900799162877\n",
      "Validation accuracy = 0.9391025641025641\n",
      "EPOCH 9 ...\n",
      "Validation loss = 0.18551269441078871\n",
      "Validation accuracy = 0.9457131410256411\n",
      "EPOCH 10 ...\n",
      "Validation loss = 0.1690909312799191\n",
      "Validation accuracy = 0.9517227564102564\n",
      "Test loss = 0.1736905741123244\n",
      "Test accuracy = 0.9486177884615384\n"
     ]
    }
   ],
   "source": [
    "# MNIST consists of 28x28x1, grayscale images.\n",
    "x = tf.placeholder(tf.float32, (None, 784))\n",
    "# Classify over 10 digits 0-9.\n",
    "y = tf.placeholder(tf.float32, (None, 10))\n",
    "# Create the LeNet.\n",
    "fc2 = LeNet(x)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(fc2, y))\n",
    "opt = tf.train.AdamOptimizer()\n",
    "train_op = opt.minimize(loss_op)\n",
    "correct_prediction = tf.equal(tf.argmax(fc2, 1), tf.argmax(y, 1))\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "def eval_data(dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset as input returns the loss and accuracy.\n",
    "    \"\"\"\n",
    "    steps_per_epoch = dataset.num_examples // BATCH_SIZE\n",
    "    num_examples = steps_per_epoch * BATCH_SIZE\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for step in range(steps_per_epoch):\n",
    "        batch_x, batch_y = dataset.next_batch(BATCH_SIZE)\n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={x: batch_x, y: batch_y})\n",
    "        total_acc += (acc * batch_x.shape[0])\n",
    "        total_loss += (loss * batch_x.shape[0])\n",
    "    return total_loss/num_examples, total_acc/num_examples\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load data\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        steps_per_epoch = mnist.train.num_examples // BATCH_SIZE\n",
    "        num_examples = steps_per_epoch * BATCH_SIZE\n",
    "\n",
    "        # Train model\n",
    "        for i in range(EPOCHS):\n",
    "            for step in range(steps_per_epoch):\n",
    "                batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)\n",
    "                loss = sess.run(train_op, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "            val_loss, val_acc = eval_data(mnist.validation)\n",
    "            print(\"EPOCH {} ...\".format(i+1))\n",
    "            print(\"Validation loss = {}\".format(val_loss))\n",
    "            print(\"Validation accuracy = {}\".format(val_acc))\n",
    "\n",
    "        # Evaluate on the test data\n",
    "        test_loss, test_acc = eval_data(mnist.test)\n",
    "        print(\"Test loss = {}\".format(test_loss))\n",
    "        print(\"Test accuracy = {}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
